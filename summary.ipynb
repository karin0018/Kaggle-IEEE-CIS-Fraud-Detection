{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle IEEE-CIS Fraud Detection\n",
    "\n",
    "本文主要介绍了一些自己在 Kaggle 上首战的经验，包括数据分析，特征工程，模型选择与训练等内容。  \n",
    "\n",
    "首先拿到一个比赛题目，先明确题意，即要解决的问题，这里分为两部分：  \n",
    "1. 这个比赛需要我们解决什么现实问题？  \n",
    "> 这明确了我们可能需要用到的相关领域知识，以及对后续的特征构建或有启发意义；\n",
    "2. 这是什么类型的机器学习任务？有监督/无监督，分类/回归，是否需要处理图片和文本等等。  \n",
    "> 这决定我们需要用哪些方式做数据清理、分析，以及用什么模型学习特征。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其次，对数据集进行初步探索和分析(EDA)，这一过程包括清洗数据和可视化分析，为后续提取和构建特征做铺垫；     \n",
    "\n",
    "之后，是最重要的特征工程(FE)，在这一过程中，需要不断尝试、测试新特征，除了基本的特征提取方法外，经常关注讨论区也能给予自己寻找特征的灵感，这一步是为了训练模型做特征上的准备，特征的好坏能很大程度上影响模型学习的效果；  \n",
    "\n",
    "最后是选择并构建模型，学习特征并预测结果，本题选择了在其他 kernel 中广受好评的 LightGBM，用 5 折交叉验证防止过拟合并评估模型性能。   \n",
    "\n",
    "另外，本次比赛中，我还有许多不足之处，写在文末的总结中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题描述\n",
    "- 任务概述：通过给定的在线电子交易信息，预测该交易是否为欺诈交易；\n",
    "- 任务类型：有监督，二分类；\n",
    "- 数据集：\n",
    "    train/test_identity -用户身份信息；\n",
    "    train/test_transaction -交易信息；\n",
    "- 评测方式：[ROC 曲线下面积(AUC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) ;\n",
    "- 提交格式：    \n",
    "TransactionID,isFraud  \n",
    "3663549,0.5  \n",
    "3663550,0.5  \n",
    "3663551,0.5  \n",
    "etc.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在已经明确了题意 -- 有监督，二分类，需要预测欺诈性交易；  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1. 探索性数据分析 (EDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这部分主要是对数据集有一个基本的了解,对之后的特征工程起一个启发式作用。  \n",
    "\n",
    "- 比如，从整体上来说可以考虑：一共多少列特征，多少数值型特征、多少类别型特征，是否出现类别分布不均衡问题？    \n",
    "\n",
    "- 具体到每一个特征就是，考察特征分别在正样本和负样本中如何分布，有什么异同？各个特征的缺失值都有多少？ 重点关注负样本的特征数据，看看能不能发现什么具有代表性的特征等等。 \n",
    "> 对类别型特征可以计数统计，这里绘制直方图会更加直观，连续数值特征可以绘制核密度图来看其概率分布。  \n",
    "缺失值统计可以直接调用 ```df.isnull().sum()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>...</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>id_34</th>\n",
       "      <th>id_35</th>\n",
       "      <th>id_36</th>\n",
       "      <th>id_37</th>\n",
       "      <th>id_38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>samsung browser 6.2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2220x1080</td>\n",
       "      <td>match_status:2</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>mobile</td>\n",
       "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "2        2987002        0          86469            59.0         W   4663   \n",
       "3        2987003        0          86499            50.0         W  18132   \n",
       "4        2987004        0          86506            50.0         H   4497   \n",
       "\n",
       "   card2  card3       card4  card5  ...                id_31  id_32  \\\n",
       "0    NaN  150.0    discover  142.0  ...                  NaN    NaN   \n",
       "1  404.0  150.0  mastercard  102.0  ...                  NaN    NaN   \n",
       "2  490.0  150.0        visa  166.0  ...                  NaN    NaN   \n",
       "3  567.0  150.0  mastercard  117.0  ...                  NaN    NaN   \n",
       "4  514.0  150.0  mastercard  102.0  ...  samsung browser 6.2   32.0   \n",
       "\n",
       "       id_33           id_34  id_35 id_36 id_37  id_38  DeviceType  \\\n",
       "0        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "1        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "2        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "3        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "4  2220x1080  match_status:2      T     F     T      T      mobile   \n",
       "\n",
       "                      DeviceInfo  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4  SAMSUNG SM-G892A Build/NRD90M  \n",
       "\n",
       "[5 rows x 434 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能看到现在有 434 列特征，面对如此高维的特征，能想到我们在特征提取上会有很多工作要做。下面结合讨论区对各个特征的解释，来观察各列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原始特征\n",
    "- 交易信息特征：\n",
    "    - ```TransactionDT```：给定参考日期时间的时间戳（不是实际时间戳）\n",
    "    - ```TransactionAMT```：以美元为单位的交易支付金额\n",
    "    - **```ProductCD```**：产品或服务类型\n",
    "    - **```card1-card6```**：支付卡信息，例如卡类型，卡类别，发行银行，国家等\n",
    "    - **```addr1/2```**：交易地址\n",
    "    - ```dist```：距离\n",
    "    - **```P_/R__```** :电子邮件域名：买方和收件人电子邮件域名\n",
    "    - ```C1-C14```：计数，例如发现与支付卡关联的地址数等。实际含义被屏蔽\n",
    "    - ```D1-D15```：时间间隔，例如与前一次交易之间的天数等\n",
    "    - **```M1-M9```**：个人信息是否匹配，例如卡上的姓名和地址是否匹配等\n",
    "    - ```Vxxx```：其他的特征\n",
    "- 身份信息特征：\n",
    "    - **```DeviceType```**：交易设备\n",
    "    - **```DeviceInfo```**：交易设备\n",
    "    - ```id_01 - id_11```：身份的数字特征\n",
    "    - **```id_12 - id_38```**：身份的类别特征 \n",
    "    \n",
    "> 加粗黑体为类别型特征     \n",
    "\n",
    "官方给出的标记规则：    \n",
    "\n",
    "Labeling logic   \n",
    "\n",
    "\"The logic of our labeling is define reported chargeback on the card as fraud transaction (isFraud=1) and transactions posterior to it with either **user account**, *email address or billing address directly linked to these attributes as fraud too*. If none of above is reported and found beyond 120 days, then we define as legit transaction (isFraud=0)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现，一共 434 列特征 V 系列特征就足足占了 341 列！猜想这个系列的特征大概会存在很多冗余列，我们在特征提取的时候可以进行必要的降维；   \n",
    "  \n",
    "  \n",
    "另外，我们还可以注意到 C系列、D 系列、M 系列 和 id 系列特征的具体含义都不清晰，如果能够通过特征工程进一步猜想出系列特征的具体含义，或许对构建特征有启发意义。  \n",
    "\n",
    "关于标记规则，自己在参加比赛的时候并没有特别注意这里，只是按照常见的方法做特征工程，直到比后来看到比赛排名十分靠前的选手公布他们的方案时，才发现这里其实可以说大有玄机。是启发 \"构建特征 + 选择特征\" 的关键， 具体写在文末总结中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 接下来思考的问题\n",
    "1. 类别型特征如何处理，数值型数据如何处理？  \n",
    "    - 针对类别型特征，有几种编码方式可以将其转换成数值类型，方便模型学习。具体采用哪种编码方式，需要具体分析，见后文；   \n",
    "\n",
    "    - 对于数值型特征，在后续的特征工程中做了总结，在特征工程之前，可以先通过转换数值类型的方式减少数据占用的内存，提高之后模型的训练速度；  \n",
    "    eg. 将所有 float64 转换为 float32 ，将所有 int64 转换为 int32。（最好避免使用 float16。如果愿意，可以使用 int8 和 int16 ）\n",
    ">trick: 自己没有思路的时候，可以多看别人的 kernel 并关注讨论区，我发现大家对新手都很友好，即使问大神很基础的问题，他们也都会耐心解答，是真的很 nice :D\n",
    "2. 正负样本分布是否均衡，是否需要重采样？   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.96501\n",
       "1    0.03499\n",
       "Name: isFraud, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"isFraud\"].value_counts(normalize=\"TabErrorrue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Target variable count')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 正负样本分布\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "x=train['isFraud'].value_counts().values\n",
    "sns.barplot([0,1],x)\n",
    "plt.title('Target variable count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 当正负样本分布不均衡(通常把样本类别比超过 4:1 的数据称为不均衡数据)时，分类模型会存在严重的**偏向性**，举一个极端的例子，如果以准确率作为验证指标，当模型预测所有的样本都是负样本时，准确率可以达到 0.96501 ！这个时候显然是发现不了自己模型问题的，但如果测试集中给出一个正负样本分布较为均衡的数据集，所得结果可想而知。  \n",
    "- 解决方法：\n",
    "    - **调整衡量模型的指标**。即，不能再用「分类正确率」（accuracy）来衡量模型性能，而要用少数类的「准确率」（precision）和「召回率」（recall），或者二者的综合（F1, equal error rate, area under curve 等等）。本题中使用的 AUC 值其实并不能很好的规避样本不均衡的弊端；  \n",
    "    - **交叉验证**√，提高模型鲁棒性；  \n",
    "    - **数据集重采样** -- 对样本占比小的类别过抽样，样本占比大的类别欠抽样，这里可以参考一个 [kernel](https://www.kaggle.com/shahules/tackling-class-imbalance) ; \n",
    "    > 此种方法缺点也比较明显，比如欠抽样时会浪费一些数据，而过抽样又容易造成过拟合问题等。\n",
    "    - **使用现有的集成学习分类器**√，比如 LightGBM ，不论是二分类还是多分类，只要恰当设置参数就能解决这个问题。  \n",
    "    > 比如针对本题的二分类，设置```is_unbalance```参数为```True```或者设置```scale_pos_weight```,二者只能选一个。 设置```is_unbalance```参数为```True```时会把负样本的权重设为：正样本数/负样本数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 是否需要可视化？  \n",
    "    - 可视化对数据的分析和探索绝对会取得很棒的辅助作用，但对于新手来说，不用太执着于自己做可视化。  因为 python 的可视化工具多且全，一时间完全掌握并熟练运用对新手来说实在是不太友好。  \n",
    "    - 比赛初期就会有人共享出他的数据可视化方案了，如果是新手的话，初期不必浪费时间学习可视化内容，可以从别人的 kernel 中寻找灵感。\n",
    "\n",
    "比赛中分享出来的优秀 EDA：\n",
    "- [🕵️ IEEE Fraud Detection - First Look and EDA ](https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda)\n",
    "- [EDA and models](https://www.kaggle.com/artgor/eda-and-models/data)  \n",
    "- [fraud-complete-eda](https://www.kaggle.com/jesucristo/fraud-complete-eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2. 特征工程（FE）\n",
    "这部分包括对原始特征的预处理和在已有特征基础上构建新的特征，以利于模型的学习，最终目的是使预测结果比起直接使用原数据来说，更加准确。    \n",
    "\n",
    "这是提高分数的重要步骤:加入新特征->模型本地评估(cv 交叉验证) or feature importance ->  筛选特征...   \n",
    "\n",
    "这部分除了会应用到一些基本的数据处理方法外，还不可避免的需要依靠分析数据的经验和创造力。\n",
    "> 在 kaggle 的讨论区中，有时 kaggler 会表示自己发现了 \"magic feature\" ，该种特征会对最后的预测结果起关键性作用。因此如果能找到 \"magic feature\" ，那当然是很幸运的。通常情况下，我们会选择按常规步骤和专业领域知识不断补充和筛选特征，在这个过程中也许会发现 \"magic  feature\"。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缺失值处理\n",
    "1. 统计缺失值，每一笔交易的所有缺失值数量可以当做特征。\n",
    "2. 直接丢弃缺失值大于 90% 的特征；\n",
    "3. 插补法填充缺失值：\n",
    "    - 平均值、中位数插补，特殊值(eg.-999)插补；   \n",
    "    - K 临近距离法： 先根据欧式距离或相关分析来确定距离具有缺失数据样本最近的K个样本，将这 K 个值加权平均来估计该样本的缺失数据；\n",
    "    - 期望最大化算法(EM 算法)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异常值处理  \n",
    "1. 画图表，单一特征绘制箱型图(box plot)；两两特征绘制散点图(scatter plot);\n",
    "2. 根据公式计算异常点，套用处理缺失值的方法 √\n",
    "```python\n",
    "    # calculating mean and std(标准差) of the array\n",
    "    data_mean, data_std = np.mean(df_num), np.std(df_num)\n",
    "\n",
    "    # seting the cut line to both higher and lower values\n",
    "    # You can change this value\n",
    "    cut = data_std * 3 \n",
    "\n",
    "    #Calculating the higher and lower cut values\n",
    "    lower, upper = data_mean - cut, data_mean + cut\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征缩放  (Feature Scaling)\n",
    "- 特征缩放，是预处理阶段的关键步骤，能使不同的特征有相同的取值范围。虽然存在决策树和随机森林这种少数不需要特征缩放的机器学习类算法，但对于大部分机器学习算法和优化算法来说，如果特征都在同一数值范围内，会得到更好的结果；  \n",
    "\n",
    "**Q1: 原因及必要性?**\n",
    "1. 归一化/标准化实质是一种线性变换，线性变换有很多良好的性质，这些性质决定了对数据改变后不会造成“失效”，反而能提高数据的表现，这些性质是归一化/标准化的前提。\n",
    "2. 梯度下降方法求解最优化问题的需要：能够使等高线变圆，加快其收敛速度；\n",
    "3. 一些分类器的目标函数(损失函数)需要计算样本间的距离、方差等，如果一个特征域范围过大，则计算结果主要取决于这个特征，这与实际情况往往是相悖的;     \n",
    "\n",
    "\n",
    "**Q2：什么时候需要特征缩放？** \n",
    "1. 涉及或隐含[距离计算的方法](https://blog.csdn.net/Kevin_cc98/article/details/73742037)，包括欧式距离、马氏距离、余弦距离等。\n",
    "2. 损失函数中包含正则项；\n",
    "3. 梯度下降算法；    \n",
    "> eg. SVM, logistic regression   \n",
    "\n",
    "**Q3: 什么时候不需要？**  \n",
    "1. 与距离计算无关的概率模型；\n",
    "> eg. Naive Bayes  \n",
    "2. 基于树的模型，树中结点的选择值关注当前特征在哪里切分能使分类结果更加准确，这种划分方法只关注特征内部数据，而不关注特征之间的相对大小。\n",
    "> eg. 决策树，随机森林等。\n",
    "    \n",
    "### 标准化  (Normalize)  \n",
    "  Z-score 标准化：将特征值缩放到以 0 为中心，标准差为 1，即，标准化后的特征形式服从正态分布。  \n",
    "  另外，标准化后的数据保持了异常值中的有用信息，使得算法对异常值不太敏感，这一点归一化无法保证。 \n",
    "```python\n",
    "df[col] = (df[col]-df[col].mean())/df[col].std()\n",
    "```\n",
    "### 归一化  (Standardize)  \n",
    "将特征范围缩放到 $[0,1]$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# sklearn 提供了 MinMaxScaler 类实现归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train) # 注意只用训练集数据 fit 一遍，以保证训练集和测试集的缩放程度相同\n",
    "X_test_norm = mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征变换    \n",
    "对类别型特征，可以选择告诉 LightGBM 他们是类别型特征。如果预先对类别特征有数值编码操作(比如 card1)，也可以告诉 LightGBM 他们是数值型特征。虽然即使不这么做， LightGBM 也能自动抽取类别型特征，但我们可以两者都尝试并看 CV 的验证分数，取其高者为用。  \n",
    "```train[col] = train[col].astype(\"category\")```\n",
    "###  Log Transformation    \n",
    "\n",
    "针对数值型，特别是极差特别大的数值型特征，可以对其取 log。因为 x 越大，log(x) 就增长的越缓慢，取对数之后可以让数据的分布更加平稳，同时也能使计算变得简单。 \n",
    "```python\n",
    "train['TransactionAmt_Log'] = np.log(train['TransactionAmt'])\n",
    "test['TransactionAmt_Log'] = np.log(test['TransactionAmt'])  \n",
    "```\n",
    "类似的数学变换还有根式运算等；  \n",
    "\n",
    "###  Label Encoding    \n",
    "对离散的数字或文本特征编码，当特征的各个类别之间存在一定的逻辑时，选用 Label Encoding 会更加合适，因为编码的数字顺序可以与原有逻辑对应，而不破坏它。\n",
    "\n",
    "###  One-hot Encoding   \n",
    "独热编码：又称一位有效编码，用 N 维向量来对 N 个状态编码，在任意情况下，该 N 维向量只有一位有效。\n",
    "- 优点：\n",
    "    - 解决了一些分类器不好处理类别型数据的问题；\n",
    "    - 在一定程度上也起到了扩征的作用；\n",
    "- 缺点：\n",
    "    - 类别过多时容易产生维数灾难；\n",
    "    - 对于基于树的模型来说，本质是在增加树的深度，所以并不适合基于树的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 对类别型特征数值化编码\n",
    "for col in cat_cols:\n",
    "    if col in train.columns:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n",
    "        train[col] = le.transform(list(train[col].astype(str).values))\n",
    "        test[col] = le.transform(list(test[col].astype(str).values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征构建 (Feature Construction)    \n",
    "指从原始的特征中，人工创造出新的特征。   \n",
    "这部分个人认为主要靠数据分析的经验 + 题目涉及领域的专业知识 + 与其他参赛者的交流；\n",
    "\n",
    "### 时间特征    \n",
    "本题中，数据集中给出的 ```TransactionDT``` 并不是具体的时间，而是与某个具体时间的间隔(单位可以猜测为 \"秒\")。所以，对```TransactionDT```，可以进一步构建出```Transaction_day_of_week```,```Transaction_hour```.    \n",
    "```python\n",
    "train['Transaction_day_of_week'] = np.floor((train['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "test['Transaction_day_of_week'] = np.floor((test['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "train['Transaction_hour'] = np.floor(train['TransactionDT'] / 3600) % 24\n",
    "test['Transaction_hour'] = np.floor(test['TransactionDT'] / 3600) % 24\n",
    "```\n",
    "###  Splitting  \n",
    "字符串列可以按照字符串的特点分裂成两个及以上列，比如邮件域名、交易金额、使用设备等。这些分裂出来的新特征信息，只靠原始数据和 LightGBM 是得不到的。  \n",
    "\n",
    "- 邮件域名  \n",
    "\n",
    "    交易信息中，买方和卖方的邮件域名都十分重要，因为有可能欺诈交易的邮件域名有某些相似之处。   \n",
    "    他们有长 ```nan```这样的，也有```hotmail.com```这样的，还有```prodigy.net.mx```这样的，于是对他们的处理就是按 ``` . ```分开，分别构建三列特征："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "source": [
    "```python  \n",
    "train[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['P_emaildomain'].str.split('.', expand=True)  \n",
    "train[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['R_emaildomain'].str.split('.', expand=True)\n",
    "```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TransactionAmt  \n",
    "    交易金额可以划分成整数部分和小数部分。  \n",
    "```python  \n",
    "train['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "```  \n",
    "- DeviceInfo   \n",
    "\n",
    "    ```id_30``` 中 ```Mac OS X 10_9_5``` ，```Mac OS X``` 是操作系统类型，```10_9_5```是该操作系统的版本。由此即可分成两列特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Encoding\n",
    "对一些类别型特征(包括缺失值)来说，可以统计各类别出现的次数，作为新的特征。\n",
    "```python\n",
    "# map 理解成映射\n",
    "train['card1_count_full'] = train['card1'].map(\n",
    "    pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False))  \n",
    "\n",
    "# 统计缺失值\n",
    "train_df['nulls1'] = train_df.isna().sum(axis=1)\n",
    "```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining / Interaction      \n",
    "\n",
    "这两种方法看似简单，实际上却可以挖掘出几个 feature 之间内在的联系，比如单个的特征并不能看出与 target 有多少关联，模型在学习的时候也就不会重视这些特征，但十分有可能的是，两个特征联合起来就与 target 的判断其很大作用。  \n",
    "\n",
    "- Combining    \n",
    "主要针对 categorical 特征，把两个或以上的特征通过某种方式结合在一起.。  \n",
    "比如直接联合：  \n",
    "\n",
    "```python \n",
    "# Some arbitrary features interaction\n",
    "for feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2',\n",
    "                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n",
    "    f1, f2 = feature.split('__')\n",
    "    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n",
    "    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n",
    "\n",
    "```    \n",
    "\n",
    "- Interaction    \n",
    "主要针对数值型特征。可以用 A + B , A - B, A * B ,A/B 等基本数学运算建立新特征；   \n",
    "\n",
    "```python\n",
    "train[\"x1_x2\"] = train[\"x1\"] * train[\"x2\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征聚合    (Aggregations/ Group Statistics)\n",
    "这里引用一段大神 ([Chris Deotte](https://www.kaggle.com/cdeotte)) 在讨论区对这个工作用处的解释：  \n",
    "\"  \n",
    "  Creating group aggregation features are great for detecting anomalies.   \n",
    "  \n",
    "  Every transaction belongs to many groups. Whenever a transaction differs from the normal within that group, it is suspicious of being fraud.  \n",
    "    \n",
    "  For example, you can encode df.groupby('addr1').TransactionAmt.agg('mean'). This is the average amount of money spent within each addr1 (i.e. zipcode). If everyone in a certain zipcode only purchases items less than ```$100``` and then one client from that zipcode purchases something for ```$1000``` that is strange and possibly fraud.   \n",
    "  \"  \n",
    "  \n",
    "  也就是说，特征分组聚合可以帮助我们的模型发现在该组中，有哪些值是 common 的，那些值是 rare 的，自然，拥有与众不同的值的样本是欺诈交易的可能性很大（因为之前已经发现所有的交易中，欺诈交易只占很少的一部分），以此来提升模型预测的准确性。\n",
    "\n",
    "```python\n",
    "train['D15_to_std_addr2'] = train['D15'] / train.groupby(['addr2'])['D15'].transform('std')\n",
    "train['TransactionDT_to_std_TransactionID']=train.groupby(['TransactionDT'])['TransactionID'].transform('std')\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取  \n",
    "特征提取是从原始特征空间抽取信息，从而构建一个新的特征子空间。\n",
    "- 降维\n",
    "    - PCA (Principal Component Analysis) :是一种常见的数据分析方式，常用于高维数据的降维，可用于提取数据的主要特征分量；  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 对 v 系列特征 PCA 降维\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "## 特征选择  \n",
    "特征选择从原始体征集中选择一个子集合。  \n",
    "> 删除不必要的列:\n",
    "    - 该特征中全部都只有一个值；\n",
    "    - 该特征中 90% 以上是 nan ；\n",
    "    - 该特征中有一个值出现 90% 以上；  \n",
    "    \n",
    "### Filter Method  \n",
    "过滤法：采用某一种评估方法(相关性，假设检验等），单独衡量每一个自变量与因变量之间的关系，给出每一个自变量对因变量的影响程度。整个过程没有用到模型。  \n",
    "\n",
    "### Wrapper Method   \n",
    "包装法：将特征选择算法作为学习算法的一个组成部分，并且直接使用分类性能作为特征重要性程度的评价标准。  \n",
    "以递归式特征消除为例，递归式特征消除的目标是通过递归的考虑越来越小的特征集来选择特征，知道符合特征数量满足要求为止。缺点是时间复杂度较高。\n",
    "> [交叉验证递归式特征消除](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV) √\n",
    "```python\n",
    "from sklearn.feature_selection import RFECV\n",
    "```     \n",
    "### Embedded Method   √\n",
    "嵌入法，也是用机器学习的方法来选择特征，但和 wrapper 的方法的区别在于，不是通过一些迭代方法选择特征子集，而是直接使用原特征全集训练之后，得到特征系数 coef 或者特征重要度 feature importance ，取前 k 个特征作为特征子集。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 特征学习\n",
    "- GBDT\n",
    "- 过拟合问题的解决方法：\n",
    "    - 特征选择与特征提取；√ \n",
    "        - 包括重新清洗数据：异常点，消除冗余特征等；\n",
    "    - 交叉验证；√ \n",
    "    - [正则化](https://ljalphabeta.gitbooks.io/python-/content/selecting.html)：\"优化目标 + 正则项(惩罚项)\" -- 惩罚大权重参数，降低模型复杂度 ；\n",
    "    > - 奥卡姆剃刀原理说，在所有能解释数据的模型中，越简单的越靠谱。但是为了拟合复杂的数据，不得不采用更复杂的模型，而正则化方法就是，以一种相对比较简单的模型来拟合复杂数据的方法。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. 模型的选取与构建\n",
    "\n",
    "## 模型选择    \n",
    "\n",
    "适用于高维度特征的二分类问题  \n",
    "个人只使用单模型训练，但大多数参赛选手都会使用模型融合的方法得到更优解，这一点也写在最后的总结中。  \n",
    "\n",
    " - [LightGBM](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)     \n",
    " \n",
    "  \n",
    "  \n",
    "## 模型评估和调参    \n",
    "\n",
    "### K折交叉验证评估模型的性能      \n",
    "\n",
    "- K 折交叉验证过程：\n",
    "    - 1. 使用不重复随机抽样将原始数据随机分为 k 份；\n",
    "    - 2. k - 1 份用于模型训练，剩下的 1 份用于模型测试；\n",
    "    - 3. 将 (2) 重复 k 次;  \n",
    "    最后得到 k 个模型和他们的评估结果；\n",
    "    > k 一般取值为 5、10，数据量较大时，k 应该设定的小一些，不然导致各个模型的训练集相差不多，反而容易过拟合。  \n",
    "    \n",
    "### 超参调整 -- 网格搜索，随机搜索与贝叶斯优化  \n",
    "#### 网格搜索  \n",
    "网格搜索其实就是暴力搜索，我们事先为每个参数设定一组值，然后穷举各种参数组合，找到最好的那一组。 但这种方法太过耗时，显然不适合大样本的数据集。\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "```\n",
    "#### 随机搜索   \n",
    "与网格搜索相比，随机搜索不会尝试所有的参数组合，而是从指定的参数范围中随机采样，如果采样的样本集足够大，则得到的参数组合很有可能是最优参数或者近似于最优参数。  \n",
    "但这种方法同样不适合大样本数据集。  \n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "```\n",
    "#### 贝叶斯优化调参  √\n",
    "  \n",
    "[强大而精致的机器学习调参方法：贝叶斯优化](https://www.cnblogs.com/yangruiGB2312/p/9374377.html)  \n",
    "[kaggle-Tutorial: Bayesian optimization](https://www.kaggle.com/clair14/tutorial-bayesian-optimization)\n",
    "\n",
    "- 主要思想： 给定优化目标函数(广义的函数，只需要给出函数的输入和输出即可)，通过不断增加样本点来更新目标函数的后验分布，换句话说，就是不断考虑上一次参数的信息，从而更好的调整当前的参数。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 优化目标函数：参数是需要调整的参数，返回优化目标的分数；\n",
    "> 这里提前把数据集划分成训练集和验证集，优化目标的分数是使用训练集数据在 lightgbm 训练过后，在验证集上验证后所得的 AUC 值；    \n",
    "2. 确定参数范围；\n",
    "> 参考 lightgbm 的官方文档和已经分享出来的 kernel；  \n",
    "3. 调用 maxmine 方法，自动寻找最优参数组合；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 贝叶斯优化调参\n",
    "# pip install bayesian-optimization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型融合 -- Voting 简单投票法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在模型选择初期，尝试了 逻辑回归、SVM、决策树、朴素贝叶斯等分类模型训练数据，并对上述模型用简单投票法做了集成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结  \n",
    "\n",
    "   在经过上述特征工程之后，分数基本可以保持在 Top 50%，达成了最初参赛的目标，也就没有再进一步修改代码。  \n",
    "   \n",
    "   之后看到一个 kernel 中，把其他人分享出来的 submission 和自己的预测结果加权平均后，提交了新的 submission，排名一下提升了很多。照着这个办法，我也试了一番，名次排到 Top23%，说实话惊讶不小。虽然评论区有不少人说该方法会导致过拟合，这么做肯定不会在后续的私有集评测中取得好成绩，但在后来的私有数据集评判中，我的名次反而上升到 18%，又是一个小惊喜hhh.   \n",
    "\n",
    "   总的说来，利用别人预测好的结果总是对自己能力的提升十分有限，所以我此次比赛的实质性成绩也就是 Top50%，期待下次比赛能有更好的成绩。  \n",
    "\n",
    "下面总结一下赛后大神们分享出来的解决方案中我没有考虑到的部分：\n",
    "## 特征工程部分   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### userID  \n",
    "[排名 1st 的大神发布的分析贴](https://www.kaggle.com/c/ieee-fraud-detection/discussion/111510)  \n",
    "正如刚开始观察到的，这个数据集的特征有 430 列，目标是识别哪一笔交易是欺诈交易。那么有哪些列可以帮助我们区分用户呢？这就是创建 userID 的最初想法。  \n",
    "这里引出我们最早提到的标记规则，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### magic feature -- D 系列特征的聚合     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征选择 -- 利用时间特征 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 模型选择与构建  \n",
    "### 多模型融合  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
